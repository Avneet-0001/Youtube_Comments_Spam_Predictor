{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORazSH14yyMf4uqGqzHknk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Avneet-0001/Youtube_Comments_Spam_Predictor/blob/main/Youtube_Comments_Spam_Detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project: Youtube Comments Spam Detector"
      ],
      "metadata": {
        "id": "B660kDenopJh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7j8AA5hk0Kr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#_____Loaded csv data to pandas dataframe____(1)#\n",
        "path = \"C:/Users/Silviya/Documents/AI/final project\"\n",
        "filename = 'Youtube04-Eminem.csv'\n",
        "fullpath = os.path.join(path,filename)\n",
        "\n",
        "data = pd.read_csv(fullpath)    #Stored all data of given csv file to dataframe called 'data'\n",
        "\n",
        "\n",
        "# Import the data from github using git command\n",
        "# !git clone https://github.com/nouralshakhouri/DSND_Term1.git\n",
        "# Browsing the directory\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "jtRaoMvNlmh9",
        "outputId": "76ce9593-fe07-419c-fc2e-552bce29c395"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'C:/Users/Silviya/Documents/AI/final project/Youtube04-Eminem.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-0011f9721f5a>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfullpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfullpath\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m#Stored all data of given csv file to dataframe called 'data'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/Silviya/Documents/AI/final project/Youtube04-Eminem.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Exploration"
      ],
      "metadata": {
        "id": "dvs8po4bo80B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#_______Basic data Exploration_____(2)#\n",
        "print(data.head(3))     #Diplaying first 3 records\n",
        "print(data.shape)       #Displaying shape of dataframe"
      ],
      "metadata": {
        "id": "hSicREMil0F6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "15mVlT8ipCJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Below, kept features that are important to train model and for prediction\n",
        "#Removed 3 features(columns), COMMENT_ID, AUTHOR, DATE\n",
        "#These 3 columns will not be helpful for trainning and prediction\n",
        "to_keep = ['CONTENT','CLASS']\n",
        "data_updated= data[to_keep]"
      ],
      "metadata": {
        "id": "ihczkgBll2VK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = data_updated['CONTENT']   #Stored feature X in variable train_x\n",
        "train_y = data_updated['CLASS']     #Stored feature Y in varialbe train_y\n",
        "\n",
        "\n",
        "#_____Preparing data for modeling_____(3)#\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count_vectorizer = CountVectorizer()    #Created object of sklearn method 'CountVectorizer()'\n",
        "\n",
        "#This will create vectors to count number of occurance of all unique words by removing stop words\n",
        "CV_train_x= count_vectorizer.fit_transform(train_x)\n",
        "\n",
        "#_____Highlight of output_______(4)#\n",
        "print(f\"Shape of data after using CountVectorizer :{CV_train_x.shape}\", )\n",
        "print(CV_train_x)\n",
        "\n",
        "\n",
        "#____Transform the data using tfidfTransform____(5)#\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "#tfid stands for Term Frequency(tf) and Inverse Document Frequency(idf)\n",
        "#Tf total count of each word in a document(Here a comment) and divide it by total number of\n",
        "#words in that document(Here a same comment)\n",
        "\n",
        "#idf check total number of documents(Here comments) that consist given word and divide it by\n",
        "#total number of all documents(Here all comments)\n",
        "\n",
        "tfidf = TfidfTransformer()\n",
        "tfidf_train_x= tfidf.fit_transform(CV_train_x)\n",
        "\n",
        "\n",
        "print(type(tfidf_train_x))  #It will show type of data after transforming it by tfidf\n",
        "print(tfidf_train_x.shape)  #Shape of data\n",
        "\n",
        "#_____Shuffle data_____(6)#\n",
        "dataframe_shuf = pd.DataFrame(tfidf_train_x.toarray(), columns= count_vectorizer.get_feature_names())\n",
        "dataframe_shuf['CLASS'] = train_y\n",
        "dataframe_shuf = dataframe_shuf.sample(frac = 1)\n",
        "\n",
        "\n",
        "#_____Splited data into 75%-train and 25%- test without using train_test_split___(7)#\n",
        "\n",
        "data_train = round(dataframe_shuf.shape[0] * (75/100))\n",
        "x_data_train  = dataframe_shuf.iloc[:data_train, :-1]\n",
        "y_data_train = dataframe_shuf.iloc[:data_train, -1]\n",
        "\n",
        "x_data_test = dataframe_shuf.iloc[data_train:, :-1]\n",
        "y_data_test = dataframe_shuf.iloc[data_train :, -1]\n",
        "#above, slice feature can also be used to split data. [start point: end point: selection]\n",
        "\n"
      ],
      "metadata": {
        "id": "xCgE9GA0lxAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model development"
      ],
      "metadata": {
        "id": "2rTAqGnVpPD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#___Trained model using Naive Bayes____(8)#\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "classifier = MultinomialNB().fit(x_data_train, y_data_train)\n",
        "\n",
        "\n",
        "#_____Cross validated trainning data with 5 fold_____(9)#\n",
        "from sklearn.model_selection import cross_val_score\n",
        "scores = cross_val_score( MultinomialNB(), x_data_train, y_data_train, scoring='accuracy', cv=5)\n",
        "\n",
        "print(f\"Mean result of accuracy is: {scores.mean()}\" ) #Display mean score of 5 folds\n",
        "print(scores)   #display score of all 5 folds\n",
        "\n",
        "\n",
        "#____Testing model by 25% data____#\n",
        "y_data_pred = classifier.predict(x_data_test)  #Prediction by classifier(which is our model working on Naive Bayes classifier.)\n",
        "\n",
        "#____Print accuracy and confusion matrix of model_____(10)#\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "accuracy =  accuracy_score(y_data_test, y_data_pred)\n",
        "print( f\"Accuray of Model :{accuracy} \")\n",
        "\n",
        "print(\"Confusion matrix :\")\n",
        "print(confusion_matrix(y_data_test,y_data_pred))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#___Added 6 new comments ___(11)#\n",
        "new_comments =[\"Eminem doesn't try to keep up with the lyrics its the lyrics that try to keep up with him\",\n",
        "            'Nice song',\n",
        "            'if this was an album, no doubt it would be his best',\n",
        "            'Click on the link below and get 78 percent off on any electronics item',\n",
        "            'Free shoes online application. Offer for limited period',\n",
        "            'Eminen is the king of kings of all time his a Legendary Rapper'\n",
        "\n",
        "            ]\n",
        "\n",
        "#Transformed new comments\n",
        "cv_new =  count_vectorizer.transform(new_comments)\n",
        "tfidf_new = tfidf.transform(cv_new)\n",
        "pred = classifier.predict(tfidf_new)\n",
        "\n",
        "#__Compared Prediction____(12)#\n",
        "predction_correct = np.sum((pred == [0, 0, 0, 1, 1, 0]))\n",
        "print(f\"Number of correct predictions: {predction_correct}\" )\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "d4VcYpaVpRAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation"
      ],
      "metadata": {
        "id": "NhwyMIMspUJQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion"
      ],
      "metadata": {
        "id": "joQ4yEhBpWeK"
      }
    }
  ]
}